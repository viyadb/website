{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nViyaDB is in-memory columnar analytical data store, featuring:\n\n\n\n\nFast ad-hoc analytical queries\n\n\nRandom access update pattern\n\n\nBuilt-in cardinality protection\n\n\nReal-time query compilation into machine code\n\n\nDynamic period based rollup\n\n\nREST API interface with intuitive JSON-based language\n\n\nBasic SQL (DML) support\n\n\n\n\nUse cases\n\n\n\n\nCustomer-facing applications that serve ad-hoc analytical queries (aggregations).\n\n\nIncoming events are not organized by any column, thus random updates are very common.\n\n\n\n\nQuickstart\n\n\n\n\nSee real world use cases, and how ViyaDB solves them in \nSamples\n section.\n\n\nLearn more about ViyaDB through \nUsage\n section.", 
            "title": "Overview"
        }, 
        {
            "location": "/#overview", 
            "text": "ViyaDB is in-memory columnar analytical data store, featuring:   Fast ad-hoc analytical queries  Random access update pattern  Built-in cardinality protection  Real-time query compilation into machine code  Dynamic period based rollup  REST API interface with intuitive JSON-based language  Basic SQL (DML) support", 
            "title": "Overview"
        }, 
        {
            "location": "/#use-cases", 
            "text": "Customer-facing applications that serve ad-hoc analytical queries (aggregations).  Incoming events are not organized by any column, thus random updates are very common.", 
            "title": "Use cases"
        }, 
        {
            "location": "/#quickstart", 
            "text": "See real world use cases, and how ViyaDB solves them in  Samples  section.  Learn more about ViyaDB through  Usage  section.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/usage/", 
            "text": "Usage\n\n\nThe following section describes how to configure and run a ViyaDB instance on a single node.\n\n\nGeneral\n\n\nInteraction with ViyaDB instance is performed using REST API. Sometimes, it doesn't look like REST (See \nData Ingestion\n or \nQuerying\n sections below), but it can always be thought as a resource you're sending a request to is an action itself.\n\n\nConfiguring DB Instance\n\n\nThis section explains how to configure single instance of ViyaDB. This is not needed when running in a \nclustered\n environment.\n\n\nStore descriptor preresents a configuration file of a single ViyaDB instance. The format is the following:\n\n\n{\n\n  \nquery_threads\n:\n \n1\n,\n\n  \ncpu\n:\n \n[\n \n...\n \n],\n\n  \nworkers\n:\n \n1\n,\n\n  \nhttp_port\n:\n \n5000\n,\n\n  \ntables\n:\n \n[\n \n...\n \n],\n\n  \nstatsd\n:\n \n{\n\n    \nhost\n:\n  \n...\n \n,\n\n    \nport\n:\n \n8125\n,\n\n    \nprefix\n:\n \nviyadb.%h.\n\n  \n},\n\n  \nsupervise\n:\n \nfalse\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nquery_threads - Optional number of threads that will serve queries, defaults to 1.\n\n\ncpu_list - Optional list of zero-based CPU indices that this process will use, defaults to all available CPUs.\n\n\nworkers - Optional number of workers to start, defaults to the number of available CPUs.\n\n\nport - All workers will be assigned a different port number, starting from this one (defaults to 5000).\n\n\ntables - List of \ntable descriptors\n.\n\n\nstatsd - If specified, some metrics will be reported to the given Statsd host.\n\n\nsupervise - Optionally, run a supervisor on top of worker processes. This setting must be \ntrue\n if number of workers is greater than 1.\n\n\n\n\nCreating Tables\n\n\nTable descriptors can be either a part of \nstore descriptor\n, or they can be created on-demand using REST API.\n\n\n{\n\n  \nname\n:\n \ntable name\n,\n\n  \ndimensions\n:\n \n[\n \n...\n \n],\n\n  \nmetrics\n:\n \n[\n \n...\n \n],\n\n  \nwatch\n:\n \n{\n\n    \ndirectory\n:\n \npath to files\n,\n\n    \nextensions\n:\n \n[\n.tsv\n]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nname - Table name\n\n\ndimensions - List of \ndimension descriptors\n\n\nmetrics - List of \nmetric descriptors\n\n\nwatch - Optional configuration that enables watching directory for new files, and loading them automatically.\n\n\n\n\nTo create a table, issue the following command:\n\n\ncurl --data-binary @table.json http://\nviyadb-host\n:\nviyadb-port\n/tables\n\n\n\n\n\n\n\nExplanation in SQL terms\n\n\nAn analogy can be drawn between ViyaDB tables and SQL aggregation queries.\nFor example, consider the following SQL statement:\n\n\nsql\nSELECT app_id, SUM(revenue) FROM events GROUP BY app_id\n\n\nThis example translates to ViyaDB table \nevents\n that has a single dimension \napp_id\n and a single metric \nrevenue\n.\n\n\n\n\nDimensions\n\n\nThere are four types of dimensions:\n\n\n\n\nString\n\n\nNumeric\n\n\nBoolean\n\n\nTime\n\n\nMicrotime\n\n\n\n\nString Dimension\n\n\nString dimension is a basic one, and it's used to describe things like: country, user agent, event name, etc.\nDescription format is as follows:\n\n\n{\n\n  \nname\n:\n \ndimension name\n,\n\n  \nfield\n:\n \ninput field name\n,\n\n  \ntype\n:\n \nstring\n,\n\n  \nlength\n:\n \n...\n \n,\n\n  \ncardinality\n:\n \n...\n \n,\n\n  \ncardinality_guard\n:\n \n{\n\n    \ndimensions\n:\n \n[\nother dimension\n,\n \n...\n \n],\n\n    \nlimit\n:\n  \n...\n \n  \n}\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nname - Column name\n\n\nfield - Use alternative input field name (defaults to the column name itself)\n\n\ntype - Must be \nstring\n (or can be omitted, since it's default)\n\n\nlength - Optionally, specify maximum length for a column value (values exceeding this limit will be stripped).\n\n\ncardinality - Number of distinct values this column holds (optional, but it's recommended to set)\n\n\n\n\ncardinality_guard\n allows defining a rule of how many distinct values is it possible to store per set\nof other dimensions. For example, we can decide to store at most 200 distinct event names per event date, per country.\nAll other events will be accounted still, but they will be marked as \n__exceeded\n. This is really important option,\nespecially when incoming events are not controlled by yourself, and you don't want your database memory to explode because\nsomeone decided to sent some random values.\n\n\n\n\nCardinality protection\n\n\nCardinality protection is built into ViyaDB, which basically means that you can (and should) define the maximum number of distinct elements of any given dimension. This not only allows for filtering out irrelevant values (while still keeping record of their metrics as \"Other\"), but also makes possible doing optimizations that improve database performance and save memory.\n\n\nDimension cardinality can be applied either on a dimension independently or based on a set of other dimensions. For instance, you can disallow more than 100 different event names coming from a single mobile application per single day.\n\n\n\n\nNumeric Dimension\n\n\nThis dimension allows to store numbers as a non-metric column. Column description format is\nthe following:\n\n\n{\n\n  \nname\n:\n \ndimension name\n,\n\n  \nfield\n:\n \ninput field name\n,\n\n  \ntype\n:\n \ntype\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nname - Column name\n\n\nfield - Use alternative input field name (defaults to the column name itself)\n\n\ntype - Numeric type (see below)\n\n\n\n\nSupported numeric types are:\n\n\n\n\nbyte (-128 to 127)\n\n\nubyte (0 to 255)\n\n\nshort (-32768 to 32767)\n\n\nushort (0 to 65535)\n\n\nint (-2147483648 to 2147483647)\n\n\nuint (0 to 4294967295)\n\n\nlong (-9223372036854775808 to 9223372036854775807)\n\n\nulong (0 to 18446744073709551615)\n\n\nfloat (floating point 32 bit number)\n\n\ndouble (floating point 64 bit number)\n\n\n\n\n\n\nDeprecation note\n\n\nPreviously, only \nuint\n and \nulong\n types were supported through \nnumeric\n and \nmax\n specifications.\nYou should upgrade to the new numeric types if you're still using the old way.\n\n\n\n\nBoolean Dimension\n\n\nStores either boolean value of 'true' or 'false'.\n\n\n{\n\n  \nname\n:\n \ndimension name\n,\n\n  \nfield\n:\n \ninput field name\n,\n\n  \ntype\n:\n \nboolean\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nname - Column name\n\n\nfield - Use alternative input field name (defaults to the column name itself)\n\n\ntype - Must be \nboolean\n.\n\n\n\n\nTime and Microtime Dimensions\n\n\nThis dimension allows to store UTC time. The difference between the two is that \ntime\n precision is up to seconds, while\n\nmicrotime\n precision is up to microseconds.\n\n\n{\n\n  \nname\n:\n \ndimension name\n,\n\n  \nfield\n:\n \ninput field name\n,\n\n  \ntype\n:\n \ntime|microtime\n,\n\n  \nformat\n:\n  \n...\n \n,\n\n  \ngranularity\n:\n \ntime unit\n,\n\n  \nrollup_rules\n:\n \n[\n \n...\n \n]\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nname - Column name\n\n\nfield - Use alternative input field name (defaults to the column name itself)\n\n\ntype - The type is set according to the required precision\n\n\nformat - See below\n\n\ngranularity - When specified, this time granularity will be used for rolling up events during data ingestion\n\n\nrollup_rules - Rules for dynamic period based rollup\n\n\n\n\nParse format is one of the following:\n\n\n\n\nposix - Input time is in POSIX time format (seconds since UNIX epoch)\n\n\nmillis - Input time is in milliseconds since UNIX epoch\n\n\nmicros - Input time is in microseconds since UNIX epoch\n\n\nAny other string that uses \nstrptime\n modifiers for describing format\n\n\n\n\nSupported time units:\n\n\n\n\nyear\n\n\nmonth\n\n\nweek\n\n\nday\n\n\nhour\n\n\nminute\n\n\nsecond\n\n\n\n\nDynamic rollup rules are defined using the following format:\n\n\n{\n\n  \nafter\n:\n \nnum\n \ntime unit\n\n  \ngranularity\n:\n \ntime unit\n\n\n}\n\n\n\n\n\n\nFor example, if the rules are:\n\n\n[{\n\n   \nafter\n:\n \n3 month\n,\n\n   \ngranularity\n:\n \nweek\n\n \n},\n \n{\n\n   \nafter\n:\n \n1 year\n,\n\n   \ngranularity\n:\n \nmonth\n\n\n}]\n\n\n\n\n\n\nThen events time column will change granularity dynamically to \nweekly\n after 3 months, to \nmonthly\n after 1 year.\n\n\nMetrics\n\n\nThere are three supported metric types:\n\n\n\n\nValue\n\n\nCount\n\n\nBitset\n\n\n\n\nValue Metric\n\n\nValue metric is a numeric value combined with an aggregation function.  List of supported numeric types:\n\n\n\n\nbyte (-128 to 127)\n\n\nubyte (0 to 255)\n\n\nshort (-32768 to 32767)\n\n\nushort (0 to 65535)\n\n\nint (-2147483648 to 2147483647)\n\n\nuint (0 to 4294967295)\n\n\nlong (-9223372036854775808 to 9223372036854775807)\n\n\nulong (0 to 18446744073709551615)\n\n\nfloat (floating point 32 bit number)\n\n\ndouble (floating point 64 bit number)\n\n\n\n\nSupported functions:\n\n\n\n\nsum\n\n\nmax\n\n\nmin\n\n\navg\n\n\n\n\nThe format of defining a value metric is the following:\n\n\n{\n\n  \nname\n:\n \nmetric name\n,\n\n  \nfield\n:\n \ninput field name\n,\n\n  \ntype\n:\n \nvalue_type\n_\nfunction\n\n\n}\n\n\n\n\n\n\nCount Metric\n\n\nThis type of metric just counts number of incoming rows. To define it, use the following format:\n\n\n{\n\n  \nname\n:\n \nmetric name\n,\n\n  \nfield\n:\n \ninput field name\n,\n\n  \ntype\n:\n \ncount\n\n\n}\n\n\n\n\n\n\nBitset Metric\n\n\nThis metric allows storing numeric values in a memory optimized set structure, which supports\noperations like \nintersect\n and \nunion\n. This makes possible to run queries like: \"what is a value\ncardinality for a given set of dimensions filtered by a perdicate?\". For instance: counting unique\nmobile app users, which installed an application on last month split by country.\n\n\nMetric format:\n\n\n{\n\n  \nname\n:\n \nmetric name\n,\n\n  \nfield\n:\n \ninput field name\n,\n\n  \ntype\n:\n \nbitset\n\n\n}\n\n\n\n\n\n\nData Ingestion\n\n\nLoading from TSV files\n\n\nSave load descriptor into \nload.json\n file:\n\n\n{\n\n  \ntable\n:\n \ntarget_table\n,\n\n  \nformat\n:\n \ntsv\n,\n\n  \ntype\n:\n \nfile\n,\n\n  \ncolumns\n:\n \n[\n...\n],\n\n  \nfile\n:\n \n/path/to/data.tsv\n\n\n}\n\n\n\n\n\n\nPost this load descriptor to a running ViyaDB instance:\n\n\ncurl --data-binary @load.json http://\nviyadb-host\n:\nviyadb-port\n/load\n\n\n\n\n\nImportant notes:\n\n\n\n\nThe \ndata.tsv\n file must be accessible to the ViyaDB instance you're loading the data into.\n\n\nIf \ncolumns\n parameter is not provided, order of columns in .tsv file must be as follows: first dimensions, then metrics as they appear in the table descriptor.\n\n\ncolumns\n parameter must be used whenever one of table column defines \nfield\n attribute.\n\n\n\n\nQuerying\n\n\nSupported query types are:\n\n\n\n\nAggregate\n\n\nSearch\n\n\n\n\nTo query, a query request must be submitted using POST method:\n\n\ncurl --data-binary @query.json http://\nviyadb-host\n:\nviyadb-port\n/query\n\n\n\n\n\nRead further to learn more on different query types.\n\n\n\n\nQuery compilation\n\n\nThe first time a query is introduced to ViyaDB, a highly optimized C++ source code is generated, and the compiled version of this code is used for physical data access. That allows to minimize branch mis-predictions, increase the level of CPU cache locality, etc. That means that the first execution of a new query type will take some extra time needed for compiling it, all the subsequent queries of the same type will use already compiled version.\n\n\n\n\nAggregate Query\n\n\nThis kind of query aggregates records selected using filter predicate, and returns them to user (optionally, sorted and/or limited). It's important to know that aggregation is done in a memory, therefore all the result set must fit in.\n\n\nQuery format:\n\n\n{\n\n  \ntype\n:\n \naggregate\n,\n\n  \ntable\n:\n \ntable name\n,\n\n  \nselect\n:\n \n[\n \n...\n \n],\n\n  \nfilter\n:\n  \n...\n \n,\n\n  \nhaving\n:\n \n...\n,\n\n  \nsort\n:\n \n[\n \n...\n \n]\n \n,\n\n  \nskip\n:\n \n0\n,\n\n  \nlimit\n:\n \n0\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\ntable - Table name\n\n\nselect - List of parameters describing how to select a column (see below)\n\n\nfilter - Filter description (see below)\n\n\nhaving - Post-aggregation filter description (similar to SQL HAVING clause). The format is the same as in \nfilter\n attribute.\n\n\nsort - Optional result sorting configuration (see below)\n\n\nskip - Optionally, skip this number of output records\n\n\nlimit - Optionally, limit result set size to this number\n\n\n\n\nColumn Selector\n\n\nColumn is either dimension or metric. The format of selecting either of them is the following:\n\n\n{\n\n  \ncolumn\n:\n \ncolumn name\n,\n\n  \nformat\n:\n \ntime format\n,\n\n  \ngranularity\n:\n \ntime granularity\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\ncolumn - Dimension or metric name\n\n\n\n\nTime column has two additional optional parameters:\n\n\n\n\nformat - Time output format (check \nstrptime\n documentation for available modifiers). By default, UTC epoch timestamp will be sent\n\n\ngranularity - Rollup results by this time unit (see \ntime dimension\n configuration for supported time units)\n\n\n\n\nQuery Filters\n\n\nFilter is one of mandatory parameters in a query, which allows skipping irrelevant records. There are four different filter types.\n\n\nValue Operator Filter\n\n\n{\n\n  \nop\n:\n \nfilter operator\n,\n\n  \ncolumn\n:\n \ncolumn name\n,\n\n  \nvalue\n:\n \nfilter value\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nop - Filter kind specified by operator\n\n\ncolumn - Dimension or metric name\n\n\nvalue - Value that filter operates on\n\n\n\n\nSupported operators are:\n\n\n\n\neq - Equals\n\n\nne - Not equals\n\n\nlt - Less than\n\n\nle - Less or equals to\n\n\ngt - Greater than\n\n\nge - Greater or equals to\n\n\n\n\nNegation Filter\n\n\n{\n\n  \nop\n:\n \nnot\n,\n\n  \nfilter\n:\n  \n...\n \n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nop - Must be \nnot\n\n\nfilter - Other filter descriptor\n\n\n\n\nInclusion Filter\n\n\n{\n\n  \nop\n:\n \nin\n,\n\n  \ncolumn\n:\n \ncolumn name\n,\n\n  \nvalues\n:\n \n[\nvalue1\n,\n \n...\n \n]\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nop - Must be \nin\n\n\ncolumn - Dimension or metric name\n\n\nvalues - List of values to filter on\n\n\n\n\nComposite Filter\n\n\n{\n\n  \nop\n:\n \ncomposition operator\n,\n\n  \nfilters\n:\n \n[\n \n...\n \n]\n\n\n}\n\n\n\n\n\n\nParameters:\n\n\n\n\nop - One of composition operators: \nor\n, \nand\n\n\nfilters - List of other filters to compose\n\n\n\n\nFilter Example\n\n\nBelow is an example of using different filter types:\n\n\n{\n\n  \ntype\n:\n \naggregate\n,\n\n  \ntable\n:\n \nactivity\n,\n\n  \nselect\n:\n \n[\n\n    \n{\ncolumn\n:\n \ninstall_date\n},\n\n    \n{\ncolumn\n:\n \nad_network\n},\n\n    \n{\ncolumn\n:\n \ninstall_country\n},\n\n    \n{\ncolumn\n:\n \ninstalls_count\n},\n\n    \n{\ncolumn\n:\n \nlaunches_count\n},\n\n    \n{\ncolumn\n:\n \ninapps_count\n}\n\n  \n],\n\n  \nfilter\n:\n \n{\nop\n:\n \nand\n,\n \nfilters\n:\n \n[\n\n    \n{\nop\n:\n \neq\n,\n \ncolumn\n:\n \napp_id\n,\n \nvalue\n:\n \ncom.teslacoilsw.notifier\n},\n\n    \n{\nop\n:\n \nge\n,\n \ncolumn\n:\n \ninstall_date\n,\n \nvalue\n:\n \n2015-01-01\n},\n\n    \n{\nop\n:\n \nlt\n,\n \ncolumn\n:\n \ninstall_date\n,\n \nvalue\n:\n \n2015-01-30\n},\n\n    \n{\nop\n:\n \nin\n,\n \ncolumn\n:\n \ninstall_country\n,\n \nvalues\n:\n \n[\nUS\n,\n \nIL\n,\n \nRU\n]}\n\n  \n]},\n\n  \nhaving\n:\n \n{\nop\n:\n \ngt\n,\n \ncolumn\n:\n \ninapps_count\n,\n \nvalue\n:\n \n10\n}\n\n\n}\n\n\n\n\n\n\nSorting Results\n\n\nYou can ask to sort output results by set of columns, and specify sort order on each of them. Column sort configuration goes as follows:\n\n\n{\n\n   \ncolumn\n:\n \ncolumn name\n,\n\n   \nascending\n:\n \ntrue\n|\nfalse\n\n\n}\n\n\n\n\n\n\nascending\n parameter can be ommitted, in this case the sort order will be descending. \n\n\nSearch Query\n\n\nThis query type retrieves dimension values by a given set of filters. This feature can come in handy when implementing a field values type assist, when developing an analytics user interface filter.\n\n\nThe basic format is the following:\n\n\n{\n\n  \ntype\n:\n \nsearch\n,\n\n  \ntable\n:\n \ntable name\n,\n\n  \ndimension\n:\n \ndimension name\n,\n\n  \nterm\n:\n \nsearch term\n,\n\n  \nlimit\n:\n \n0\n,\n\n  \nfilter\n:\n  \n...\n\n\n}\n\n\n\n\n\n\nMetadata Queries\n\n\nThere are several endpoints that can be used for retreiving information about database status.\n\n\nDatabase Metadata Query\n\n\nThe following query returns basic information about the database:\n\n\ncurl http://\nviyadb-host\n:\nviyadb-port\n/database/meta\n\n\n\n\n\nTable Metadata Query\n\n\nTo see all available table fields, their types as long as some basic statistics\nuse the following query:\n\n\ncurl http://\nviyadb-host\n:\nviyadb-port\n/tables/\ntable name\n/meta", 
            "title": "Usage"
        }, 
        {
            "location": "/usage/#usage", 
            "text": "The following section describes how to configure and run a ViyaDB instance on a single node.", 
            "title": "Usage"
        }, 
        {
            "location": "/usage/#general", 
            "text": "Interaction with ViyaDB instance is performed using REST API. Sometimes, it doesn't look like REST (See  Data Ingestion  or  Querying  sections below), but it can always be thought as a resource you're sending a request to is an action itself.", 
            "title": "General"
        }, 
        {
            "location": "/usage/#configuring-db-instance", 
            "text": "This section explains how to configure single instance of ViyaDB. This is not needed when running in a  clustered  environment.  Store descriptor preresents a configuration file of a single ViyaDB instance. The format is the following:  { \n   query_threads :   1 , \n   cpu :   [   ...   ], \n   workers :   1 , \n   http_port :   5000 , \n   tables :   [   ...   ], \n   statsd :   { \n     host :    ...   , \n     port :   8125 , \n     prefix :   viyadb.%h. \n   }, \n   supervise :   false  }   Parameters:   query_threads - Optional number of threads that will serve queries, defaults to 1.  cpu_list - Optional list of zero-based CPU indices that this process will use, defaults to all available CPUs.  workers - Optional number of workers to start, defaults to the number of available CPUs.  port - All workers will be assigned a different port number, starting from this one (defaults to 5000).  tables - List of  table descriptors .  statsd - If specified, some metrics will be reported to the given Statsd host.  supervise - Optionally, run a supervisor on top of worker processes. This setting must be  true  if number of workers is greater than 1.", 
            "title": "Configuring DB Instance"
        }, 
        {
            "location": "/usage/#creating-tables", 
            "text": "Table descriptors can be either a part of  store descriptor , or they can be created on-demand using REST API.  { \n   name :   table name , \n   dimensions :   [   ...   ], \n   metrics :   [   ...   ], \n   watch :   { \n     directory :   path to files , \n     extensions :   [ .tsv ] \n   }  }   Parameters:   name - Table name  dimensions - List of  dimension descriptors  metrics - List of  metric descriptors  watch - Optional configuration that enables watching directory for new files, and loading them automatically.   To create a table, issue the following command:  curl --data-binary @table.json http:// viyadb-host : viyadb-port /tables   Explanation in SQL terms  An analogy can be drawn between ViyaDB tables and SQL aggregation queries.\nFor example, consider the following SQL statement:  sql\nSELECT app_id, SUM(revenue) FROM events GROUP BY app_id  This example translates to ViyaDB table  events  that has a single dimension  app_id  and a single metric  revenue .", 
            "title": "Creating Tables"
        }, 
        {
            "location": "/usage/#dimensions", 
            "text": "There are four types of dimensions:   String  Numeric  Boolean  Time  Microtime", 
            "title": "Dimensions"
        }, 
        {
            "location": "/usage/#string-dimension", 
            "text": "String dimension is a basic one, and it's used to describe things like: country, user agent, event name, etc.\nDescription format is as follows:  { \n   name :   dimension name , \n   field :   input field name , \n   type :   string , \n   length :   ...   , \n   cardinality :   ...   , \n   cardinality_guard :   { \n     dimensions :   [ other dimension ,   ...   ], \n     limit :    ...  \n   }  }   Parameters:   name - Column name  field - Use alternative input field name (defaults to the column name itself)  type - Must be  string  (or can be omitted, since it's default)  length - Optionally, specify maximum length for a column value (values exceeding this limit will be stripped).  cardinality - Number of distinct values this column holds (optional, but it's recommended to set)   cardinality_guard  allows defining a rule of how many distinct values is it possible to store per set\nof other dimensions. For example, we can decide to store at most 200 distinct event names per event date, per country.\nAll other events will be accounted still, but they will be marked as  __exceeded . This is really important option,\nespecially when incoming events are not controlled by yourself, and you don't want your database memory to explode because\nsomeone decided to sent some random values.   Cardinality protection  Cardinality protection is built into ViyaDB, which basically means that you can (and should) define the maximum number of distinct elements of any given dimension. This not only allows for filtering out irrelevant values (while still keeping record of their metrics as \"Other\"), but also makes possible doing optimizations that improve database performance and save memory.  Dimension cardinality can be applied either on a dimension independently or based on a set of other dimensions. For instance, you can disallow more than 100 different event names coming from a single mobile application per single day.", 
            "title": "String Dimension"
        }, 
        {
            "location": "/usage/#numeric-dimension", 
            "text": "This dimension allows to store numbers as a non-metric column. Column description format is\nthe following:  { \n   name :   dimension name , \n   field :   input field name , \n   type :   type  }   Parameters:   name - Column name  field - Use alternative input field name (defaults to the column name itself)  type - Numeric type (see below)   Supported numeric types are:   byte (-128 to 127)  ubyte (0 to 255)  short (-32768 to 32767)  ushort (0 to 65535)  int (-2147483648 to 2147483647)  uint (0 to 4294967295)  long (-9223372036854775808 to 9223372036854775807)  ulong (0 to 18446744073709551615)  float (floating point 32 bit number)  double (floating point 64 bit number)    Deprecation note  Previously, only  uint  and  ulong  types were supported through  numeric  and  max  specifications.\nYou should upgrade to the new numeric types if you're still using the old way.", 
            "title": "Numeric Dimension"
        }, 
        {
            "location": "/usage/#boolean-dimension", 
            "text": "Stores either boolean value of 'true' or 'false'.  { \n   name :   dimension name , \n   field :   input field name , \n   type :   boolean  }   Parameters:   name - Column name  field - Use alternative input field name (defaults to the column name itself)  type - Must be  boolean .", 
            "title": "Boolean Dimension"
        }, 
        {
            "location": "/usage/#time-and-microtime-dimensions", 
            "text": "This dimension allows to store UTC time. The difference between the two is that  time  precision is up to seconds, while microtime  precision is up to microseconds.  { \n   name :   dimension name , \n   field :   input field name , \n   type :   time|microtime , \n   format :    ...   , \n   granularity :   time unit , \n   rollup_rules :   [   ...   ]  }   Parameters:   name - Column name  field - Use alternative input field name (defaults to the column name itself)  type - The type is set according to the required precision  format - See below  granularity - When specified, this time granularity will be used for rolling up events during data ingestion  rollup_rules - Rules for dynamic period based rollup   Parse format is one of the following:   posix - Input time is in POSIX time format (seconds since UNIX epoch)  millis - Input time is in milliseconds since UNIX epoch  micros - Input time is in microseconds since UNIX epoch  Any other string that uses  strptime  modifiers for describing format   Supported time units:   year  month  week  day  hour  minute  second   Dynamic rollup rules are defined using the following format:  { \n   after :   num   time unit \n   granularity :   time unit  }   For example, if the rules are:  [{ \n    after :   3 month , \n    granularity :   week \n  },   { \n    after :   1 year , \n    granularity :   month  }]   Then events time column will change granularity dynamically to  weekly  after 3 months, to  monthly  after 1 year.", 
            "title": "Time and Microtime Dimensions"
        }, 
        {
            "location": "/usage/#metrics", 
            "text": "There are three supported metric types:   Value  Count  Bitset", 
            "title": "Metrics"
        }, 
        {
            "location": "/usage/#value-metric", 
            "text": "Value metric is a numeric value combined with an aggregation function.  List of supported numeric types:   byte (-128 to 127)  ubyte (0 to 255)  short (-32768 to 32767)  ushort (0 to 65535)  int (-2147483648 to 2147483647)  uint (0 to 4294967295)  long (-9223372036854775808 to 9223372036854775807)  ulong (0 to 18446744073709551615)  float (floating point 32 bit number)  double (floating point 64 bit number)   Supported functions:   sum  max  min  avg   The format of defining a value metric is the following:  { \n   name :   metric name , \n   field :   input field name , \n   type :   value_type _ function  }", 
            "title": "Value Metric"
        }, 
        {
            "location": "/usage/#count-metric", 
            "text": "This type of metric just counts number of incoming rows. To define it, use the following format:  { \n   name :   metric name , \n   field :   input field name , \n   type :   count  }", 
            "title": "Count Metric"
        }, 
        {
            "location": "/usage/#bitset-metric", 
            "text": "This metric allows storing numeric values in a memory optimized set structure, which supports\noperations like  intersect  and  union . This makes possible to run queries like: \"what is a value\ncardinality for a given set of dimensions filtered by a perdicate?\". For instance: counting unique\nmobile app users, which installed an application on last month split by country.  Metric format:  { \n   name :   metric name , \n   field :   input field name , \n   type :   bitset  }", 
            "title": "Bitset Metric"
        }, 
        {
            "location": "/usage/#data-ingestion", 
            "text": "", 
            "title": "Data Ingestion"
        }, 
        {
            "location": "/usage/#loading-from-tsv-files", 
            "text": "Save load descriptor into  load.json  file:  { \n   table :   target_table , \n   format :   tsv , \n   type :   file , \n   columns :   [ ... ], \n   file :   /path/to/data.tsv  }   Post this load descriptor to a running ViyaDB instance:  curl --data-binary @load.json http:// viyadb-host : viyadb-port /load  Important notes:   The  data.tsv  file must be accessible to the ViyaDB instance you're loading the data into.  If  columns  parameter is not provided, order of columns in .tsv file must be as follows: first dimensions, then metrics as they appear in the table descriptor.  columns  parameter must be used whenever one of table column defines  field  attribute.", 
            "title": "Loading from TSV files"
        }, 
        {
            "location": "/usage/#querying", 
            "text": "Supported query types are:   Aggregate  Search   To query, a query request must be submitted using POST method:  curl --data-binary @query.json http:// viyadb-host : viyadb-port /query  Read further to learn more on different query types.   Query compilation  The first time a query is introduced to ViyaDB, a highly optimized C++ source code is generated, and the compiled version of this code is used for physical data access. That allows to minimize branch mis-predictions, increase the level of CPU cache locality, etc. That means that the first execution of a new query type will take some extra time needed for compiling it, all the subsequent queries of the same type will use already compiled version.", 
            "title": "Querying"
        }, 
        {
            "location": "/usage/#aggregate-query", 
            "text": "This kind of query aggregates records selected using filter predicate, and returns them to user (optionally, sorted and/or limited). It's important to know that aggregation is done in a memory, therefore all the result set must fit in.  Query format:  { \n   type :   aggregate , \n   table :   table name , \n   select :   [   ...   ], \n   filter :    ...   , \n   having :   ... , \n   sort :   [   ...   ]   , \n   skip :   0 , \n   limit :   0  }   Parameters:   table - Table name  select - List of parameters describing how to select a column (see below)  filter - Filter description (see below)  having - Post-aggregation filter description (similar to SQL HAVING clause). The format is the same as in  filter  attribute.  sort - Optional result sorting configuration (see below)  skip - Optionally, skip this number of output records  limit - Optionally, limit result set size to this number", 
            "title": "Aggregate Query"
        }, 
        {
            "location": "/usage/#column-selector", 
            "text": "Column is either dimension or metric. The format of selecting either of them is the following:  { \n   column :   column name , \n   format :   time format , \n   granularity :   time granularity  }   Parameters:   column - Dimension or metric name   Time column has two additional optional parameters:   format - Time output format (check  strptime  documentation for available modifiers). By default, UTC epoch timestamp will be sent  granularity - Rollup results by this time unit (see  time dimension  configuration for supported time units)", 
            "title": "Column Selector"
        }, 
        {
            "location": "/usage/#query-filters", 
            "text": "Filter is one of mandatory parameters in a query, which allows skipping irrelevant records. There are four different filter types.", 
            "title": "Query Filters"
        }, 
        {
            "location": "/usage/#value-operator-filter", 
            "text": "{ \n   op :   filter operator , \n   column :   column name , \n   value :   filter value  }   Parameters:   op - Filter kind specified by operator  column - Dimension or metric name  value - Value that filter operates on   Supported operators are:   eq - Equals  ne - Not equals  lt - Less than  le - Less or equals to  gt - Greater than  ge - Greater or equals to", 
            "title": "Value Operator Filter"
        }, 
        {
            "location": "/usage/#negation-filter", 
            "text": "{ \n   op :   not , \n   filter :    ...   }   Parameters:   op - Must be  not  filter - Other filter descriptor", 
            "title": "Negation Filter"
        }, 
        {
            "location": "/usage/#inclusion-filter", 
            "text": "{ \n   op :   in , \n   column :   column name , \n   values :   [ value1 ,   ...   ]  }   Parameters:   op - Must be  in  column - Dimension or metric name  values - List of values to filter on", 
            "title": "Inclusion Filter"
        }, 
        {
            "location": "/usage/#composite-filter", 
            "text": "{ \n   op :   composition operator , \n   filters :   [   ...   ]  }   Parameters:   op - One of composition operators:  or ,  and  filters - List of other filters to compose", 
            "title": "Composite Filter"
        }, 
        {
            "location": "/usage/#filter-example", 
            "text": "Below is an example of using different filter types:  { \n   type :   aggregate , \n   table :   activity , \n   select :   [ \n     { column :   install_date }, \n     { column :   ad_network }, \n     { column :   install_country }, \n     { column :   installs_count }, \n     { column :   launches_count }, \n     { column :   inapps_count } \n   ], \n   filter :   { op :   and ,   filters :   [ \n     { op :   eq ,   column :   app_id ,   value :   com.teslacoilsw.notifier }, \n     { op :   ge ,   column :   install_date ,   value :   2015-01-01 }, \n     { op :   lt ,   column :   install_date ,   value :   2015-01-30 }, \n     { op :   in ,   column :   install_country ,   values :   [ US ,   IL ,   RU ]} \n   ]}, \n   having :   { op :   gt ,   column :   inapps_count ,   value :   10 }  }", 
            "title": "Filter Example"
        }, 
        {
            "location": "/usage/#sorting-results", 
            "text": "You can ask to sort output results by set of columns, and specify sort order on each of them. Column sort configuration goes as follows:  { \n    column :   column name , \n    ascending :   true | false  }   ascending  parameter can be ommitted, in this case the sort order will be descending.", 
            "title": "Sorting Results"
        }, 
        {
            "location": "/usage/#search-query", 
            "text": "This query type retrieves dimension values by a given set of filters. This feature can come in handy when implementing a field values type assist, when developing an analytics user interface filter.  The basic format is the following:  { \n   type :   search , \n   table :   table name , \n   dimension :   dimension name , \n   term :   search term , \n   limit :   0 , \n   filter :    ...  }", 
            "title": "Search Query"
        }, 
        {
            "location": "/usage/#metadata-queries", 
            "text": "There are several endpoints that can be used for retreiving information about database status.", 
            "title": "Metadata Queries"
        }, 
        {
            "location": "/usage/#database-metadata-query", 
            "text": "The following query returns basic information about the database:  curl http:// viyadb-host : viyadb-port /database/meta", 
            "title": "Database Metadata Query"
        }, 
        {
            "location": "/usage/#table-metadata-query", 
            "text": "To see all available table fields, their types as long as some basic statistics\nuse the following query:  curl http:// viyadb-host : viyadb-port /tables/ table name /meta", 
            "title": "Table Metadata Query"
        }, 
        {
            "location": "/samples/", 
            "text": "Samples\n\n\nThe following section contains several examples that cover most of the ViyaDB features using\nsingle database instance.\n\n\nRunning\n\n\nIt's recommended to use Docker image of ViyaDB for running the samples.\nTo launch Docker container with latest ViyaDB version, run:\n\n\ndocker run -p \n5000\n:5000 --rm -ti \n\\\n\n  -v /tmp/viyadb:/tmp/viyadb viyadb/viyadb:latest\n\n\n\n\n\nMobile Attribution Tracking\n\n\nMobile attribution tracking is a powerful marketing tool that allows measuring productivity of users, advertisement campaigns, etc.\nThe biggest question that tools like this can answer is where has advertiser's money gone, and how to optimize future campaigns.\n\n\nHow does this work? Mobile application generates events upon user action, and delivers them to a server, where these events are\nbeing joined to the attribution data received from an ad network and different partners. Finally, the data is analyzed based on\na date an application was installed. What we need to know for the sake of example is that we have events of the following types:\n\n\n\n\nClick (clicking on an advertisement)\n\n\nImpression (viewing an advertisement)\n\n\nInstall (installing an application)\n\n\nIn-App Event (custom in-app event generated by an application)\n\n\nLaunch (application was launched)\n\n\nUninstall (application was removed from a mobile phone)\n\n\n\n\nIn case of click and impression events, install date actually means click/impression date.\n\n\nStarting ViyaDB\n\n\nCreate \ntable.json\n file containing:\n\n\n{\n\n  \nname\n:\n \nactivity\n,\n\n  \ndimensions\n:\n \n[\n\n    \n{\nname\n:\n \napp_id\n},\n\n    \n{\nname\n:\n \nuser_id\n},\n\n    \n{\n\n      \nname\n:\n \nevent_time\n,\n \ntype\n:\n \ntime\n,\n\n      \nformat\n:\n \nmillis\n,\n \ngranularity\n:\n \nday\n\n    \n},\n\n    \n{\nname\n:\n \ncountry\n},\n\n    \n{\nname\n:\n \ncity\n},\n\n    \n{\nname\n:\n \ndevice_type\n},\n\n    \n{\nname\n:\n \ndevice_vendor\n},\n\n    \n{\nname\n:\n \nad_network\n},\n\n    \n{\nname\n:\n \ncampaign\n},\n\n    \n{\nname\n:\n \nsite_id\n},\n\n    \n{\nname\n:\n \nevent_type\n},\n\n    \n{\nname\n:\n \nevent_name\n},\n\n    \n{\nname\n:\n \norganic\n,\n \ncardinality\n:\n \n2\n},\n\n    \n{\nname\n:\n \ndays_from_install\n,\n \ntype\n:\n \nushort\n}\n\n  \n],\n\n  \nmetrics\n:\n \n[\n\n    \n{\nname\n:\n \nrevenue\n \n,\n \ntype\n:\n \ndouble_sum\n},\n\n    \n{\nname\n:\n \nusers\n,\n \ntype\n:\n \nbitset\n,\n \nfield\n:\n \nuser_id\n},\n\n    \n{\nname\n:\n \ncount\n \n,\n \ntype\n:\n \ncount\n}\n\n  \n]\n\n\n}\n\n\n\n\n\n\nCreate the table by running:\n\n\ncurl -d @table.json http://localhost:5000/tables \n\n\n\n\n\nGenerating Sample Data\n\n\nTo generate 100M user activity events, run the following:\n\n\nmkdir /tmp/viyadb\ndocker run --log-driver\n=\nnone --rm -ti -e \nEVENTS_NUMBER\n=\n10000000\n \n\\\n\n  -e \nOUTPUT_FORMAT\n=\ntsv viyadb/events-generator:latest \n /tmp/viyadb/data.tsv\n\n\n\n\n\nThis might take several minutes.\n\n\nLoading Data\n\n\nCreate load descriptor as follows:\n\n\n{\n\n  \ntable\n:\n \nactivity\n,\n\n  \nformat\n:\n \ntsv\n,\n\n  \ntype\n:\n \nfile\n,\n\n  \ncolumns\n:\n \n[\n\n    \napp_id\n,\n \nuser_id\n,\n \nevent_time\n,\n \ncountry\n,\n \ncity\n,\n\n    \ndevice_type\n,\n \ndevice_vendor\n,\n \nad_network\n,\n \ncampaign\n,\n\n    \nsite_id\n,\n \nevent_type\n,\n \nevent_name\n,\n \norganic\n,\n\n    \ndays_from_install\n,\n \nrevenue\n\n  \n],\n\n  \nfile\n:\n \n/tmp/viyadb/data.tsv\n\n\n}\n\n\n\n\n\n\nLoad the data by running:\n\n\ncurl -d @load.json http://localhost:5000/load\n\n\n\n\n\nQuerying\n\n\nThe following query finds top 10 applications by organic installs count:\n\n\n{\n\n  \ntype\n:\n \naggregate\n,\n\n  \ntable\n:\n \nactivity\n,\n\n  \nselect\n:\n \n[\n\n    \n{\ncolumn\n:\n \napp_id\n},\n\n    \n{\ncolumn\n:\n \ncount\n}\n\n  \n],\n\n  \nfilter\n:\n \n{\nop\n:\n \nand\n,\n \nfilters\n:\n \n[\n\n    \n{\nop\n:\n \nge\n,\n \ncolumn\n:\n \nevent_time\n,\n \nvalue\n:\n \n2015-01-01\n},\n\n    \n{\nop\n:\n \nle\n,\n \ncolumn\n:\n \nevent_time\n,\n \nvalue\n:\n \n2015-01-30\n},\n\n    \n{\nop\n:\n \neq\n,\n \ncolumn\n:\n \nevent_type\n,\n \nvalue\n:\n \ninstall\n},\n\n    \n{\nop\n:\n \neq\n,\n \ncolumn\n:\n \norganic\n,\n \nvalue\n:\n \nTrue\n}\n\n  \n]},\n\n  \nsort\n:\n \n[{\ncolumn\n:\n \ncount\n}],\n\n  \nlimit\n:\n \n10\n\n\n}\n\n\n\n\n\n\nSave the query in file \nquery.json\n, and run:\n\n\ncurl -d @query.json http://localhost:5000/query\n\n\n\n\n\nThe query can be invoked using experimental SQL support. To enter SQL interpreter shell, please run:\n\n\ndocker \nexec\n -ti \n\\\n\n  \n$(\ndocker ps \n|\n grep viyadb/viyadb:latest \n|\n awk \n{print $1}\n)\n \n\\\n\n  /opt/viyadb/bin/vsql\n\n\n\n\n\nOur previous query translates into SQL as follows:\n\n\nSELECT\n\n  \napp_id\n,\n \ncount\n\n\nFROM\n\n  \nactivity\n\n\nWHERE\n\n  \nevent_time\n \n=\n \n2015-01-01\n \nAND\n\n  \nevent_time\n \n=\n \n2015-01-30\n \nAND\n\n  \nevent_type\n=\ninstall\n \nAND\n\n  \norganic\n=\nTrue\n\n\nORDER\n \nBY\n\n  \ncount\n \nDESC\n\n\nLIMIT\n \n10\n\n\n\n\n\n\nLet's run the query:\n\n\nViyaDB\n SELECT app_id, count FROM activity WHERE event_time \n=\n \n2015-01-01\n AND event_time \n=\n \n2015-01-30\n AND \nevent_type\n=\ninstall\n AND \norganic\n=\nTrue\n ORDER BY count DESC LIMIT \n10\n\n\napp_id                                 count\n------                                 -----\ncom.skype.raider                       \n282\n\ncom.facebook.orca                      \n42\n\nnet.cleverbit.VegeFruits               \n41\n\ncom.virgil.basketball                  \n27\n\ncom.abzorbagames.poker                 \n21\n\nair.nn.mobile.app.main                 \n18\n\ncom.squareup                           \n17\n\nfreemoviesapp.com                      \n16\n\ncom.ea.spymouse_row                    \n14\n\ncom.bandainamcoent.tamagothiclassicus  \n14\n\n\nTime taken: \n0\n.004841 secs\n\n\n\n\n\nFor more example on this dataset please refer to the Blog post: \nAnalyzing Mobile Users Activity with ViyaDB\n.", 
            "title": "Samples"
        }, 
        {
            "location": "/samples/#samples", 
            "text": "The following section contains several examples that cover most of the ViyaDB features using\nsingle database instance.", 
            "title": "Samples"
        }, 
        {
            "location": "/samples/#running", 
            "text": "It's recommended to use Docker image of ViyaDB for running the samples.\nTo launch Docker container with latest ViyaDB version, run:  docker run -p  5000 :5000 --rm -ti  \\ \n  -v /tmp/viyadb:/tmp/viyadb viyadb/viyadb:latest", 
            "title": "Running"
        }, 
        {
            "location": "/samples/#mobile-attribution-tracking", 
            "text": "Mobile attribution tracking is a powerful marketing tool that allows measuring productivity of users, advertisement campaigns, etc.\nThe biggest question that tools like this can answer is where has advertiser's money gone, and how to optimize future campaigns.  How does this work? Mobile application generates events upon user action, and delivers them to a server, where these events are\nbeing joined to the attribution data received from an ad network and different partners. Finally, the data is analyzed based on\na date an application was installed. What we need to know for the sake of example is that we have events of the following types:   Click (clicking on an advertisement)  Impression (viewing an advertisement)  Install (installing an application)  In-App Event (custom in-app event generated by an application)  Launch (application was launched)  Uninstall (application was removed from a mobile phone)   In case of click and impression events, install date actually means click/impression date.", 
            "title": "Mobile Attribution Tracking"
        }, 
        {
            "location": "/samples/#starting-viyadb", 
            "text": "Create  table.json  file containing:  { \n   name :   activity , \n   dimensions :   [ \n     { name :   app_id }, \n     { name :   user_id }, \n     { \n       name :   event_time ,   type :   time , \n       format :   millis ,   granularity :   day \n     }, \n     { name :   country }, \n     { name :   city }, \n     { name :   device_type }, \n     { name :   device_vendor }, \n     { name :   ad_network }, \n     { name :   campaign }, \n     { name :   site_id }, \n     { name :   event_type }, \n     { name :   event_name }, \n     { name :   organic ,   cardinality :   2 }, \n     { name :   days_from_install ,   type :   ushort } \n   ], \n   metrics :   [ \n     { name :   revenue   ,   type :   double_sum }, \n     { name :   users ,   type :   bitset ,   field :   user_id }, \n     { name :   count   ,   type :   count } \n   ]  }   Create the table by running:  curl -d @table.json http://localhost:5000/tables", 
            "title": "Starting ViyaDB"
        }, 
        {
            "location": "/samples/#generating-sample-data", 
            "text": "To generate 100M user activity events, run the following:  mkdir /tmp/viyadb\ndocker run --log-driver = none --rm -ti -e  EVENTS_NUMBER = 10000000   \\ \n  -e  OUTPUT_FORMAT = tsv viyadb/events-generator:latest   /tmp/viyadb/data.tsv  This might take several minutes.", 
            "title": "Generating Sample Data"
        }, 
        {
            "location": "/samples/#loading-data", 
            "text": "Create load descriptor as follows:  { \n   table :   activity , \n   format :   tsv , \n   type :   file , \n   columns :   [ \n     app_id ,   user_id ,   event_time ,   country ,   city , \n     device_type ,   device_vendor ,   ad_network ,   campaign , \n     site_id ,   event_type ,   event_name ,   organic , \n     days_from_install ,   revenue \n   ], \n   file :   /tmp/viyadb/data.tsv  }   Load the data by running:  curl -d @load.json http://localhost:5000/load", 
            "title": "Loading Data"
        }, 
        {
            "location": "/samples/#querying", 
            "text": "The following query finds top 10 applications by organic installs count:  { \n   type :   aggregate , \n   table :   activity , \n   select :   [ \n     { column :   app_id }, \n     { column :   count } \n   ], \n   filter :   { op :   and ,   filters :   [ \n     { op :   ge ,   column :   event_time ,   value :   2015-01-01 }, \n     { op :   le ,   column :   event_time ,   value :   2015-01-30 }, \n     { op :   eq ,   column :   event_type ,   value :   install }, \n     { op :   eq ,   column :   organic ,   value :   True } \n   ]}, \n   sort :   [{ column :   count }], \n   limit :   10  }   Save the query in file  query.json , and run:  curl -d @query.json http://localhost:5000/query  The query can be invoked using experimental SQL support. To enter SQL interpreter shell, please run:  docker  exec  -ti  \\ \n   $( docker ps  |  grep viyadb/viyadb:latest  |  awk  {print $1} )   \\ \n  /opt/viyadb/bin/vsql  Our previous query translates into SQL as follows:  SELECT \n   app_id ,   count  FROM \n   activity  WHERE \n   event_time   =   2015-01-01   AND \n   event_time   =   2015-01-30   AND \n   event_type = install   AND \n   organic = True  ORDER   BY \n   count   DESC  LIMIT   10   Let's run the query:  ViyaDB  SELECT app_id, count FROM activity WHERE event_time  =   2015-01-01  AND event_time  =   2015-01-30  AND  event_type = install  AND  organic = True  ORDER BY count DESC LIMIT  10 \n\napp_id                                 count\n------                                 -----\ncom.skype.raider                        282 \ncom.facebook.orca                       42 \nnet.cleverbit.VegeFruits                41 \ncom.virgil.basketball                   27 \ncom.abzorbagames.poker                  21 \nair.nn.mobile.app.main                  18 \ncom.squareup                            17 \nfreemoviesapp.com                       16 \ncom.ea.spymouse_row                     14 \ncom.bandainamcoent.tamagothiclassicus   14 \n\nTime taken:  0 .004841 secs  For more example on this dataset please refer to the Blog post:  Analyzing Mobile Users Activity with ViyaDB .", 
            "title": "Querying"
        }, 
        {
            "location": "/clustering/", 
            "text": "Clustering\n\n\nEven though it's possible to run ViyaDB on a single node, it's usually required\nto run several database instances in order to be able to:\n\n\n\n\nScale in case aggregated dataset doesn't fit into a single machine's RAM.\n\n\nScale out concurrent queries by spreading them between different workers.\n\n\nFailover on data center failure.\n\n\n\n\nThis page describes how ViyaDB works when running in a cluster mode.\n\n\nArchitecture\n\n\nViyaDB implements shared nothing architecture. That means that the data is partitioned\nand spread between independent workers.\n\n\n\n\nComponents\n\n\nConsul\n\n\nConsul is used as a coordination service as well as for storing cluster configuration.\n\n\nThere were three different options for coordination service at the beginning: \nZookeeper\n,\n\nConsul\n and \netcd\n, but the choice was made in favor of Consul for its:\n\n\n\n\nInstallation and configuration simplicity.\n\n\nGreat Web interface.\n\n\nOut of the box service registration and lookup support.\n\n\n\n\nWorker\n\n\nWorker is a ViyaDB process that keeps a partition of data in RAM. Every such process is\nattached to it's own CPU core (using CPU affinity) for achieving better cache locality.\n\n\nEvery worker listens on its own HTTP port, where it provides a service for loading and\nquerying the data it keeps.\n\n\nController\n\n\nController is the main process that launches ViyaDB workers on a single node, and takes\ncare of keeping them alive. In addition, this process is responsible for coordination\nbetween database nodes using Consul. One of cluster controllers becomes a leader.\n\n\nController can act as query aggregator, which means that it can be queried directly, and\na query will be sent to workers containing required data partitions, then the result\nwill be combined and returned back to a user.\n\n\nCommunication with workers is performed using HTTP API.\n\n\nReal-Time Data Processing\n\n\nTo see the complete real-time data processing architecture based on ViyaDB cluster, please\nvisit \nnext\n section.", 
            "title": "Clustering"
        }, 
        {
            "location": "/clustering/#clustering", 
            "text": "Even though it's possible to run ViyaDB on a single node, it's usually required\nto run several database instances in order to be able to:   Scale in case aggregated dataset doesn't fit into a single machine's RAM.  Scale out concurrent queries by spreading them between different workers.  Failover on data center failure.   This page describes how ViyaDB works when running in a cluster mode.", 
            "title": "Clustering"
        }, 
        {
            "location": "/clustering/#architecture", 
            "text": "ViyaDB implements shared nothing architecture. That means that the data is partitioned\nand spread between independent workers.", 
            "title": "Architecture"
        }, 
        {
            "location": "/clustering/#components", 
            "text": "", 
            "title": "Components"
        }, 
        {
            "location": "/clustering/#consul", 
            "text": "Consul is used as a coordination service as well as for storing cluster configuration.  There were three different options for coordination service at the beginning:  Zookeeper , Consul  and  etcd , but the choice was made in favor of Consul for its:   Installation and configuration simplicity.  Great Web interface.  Out of the box service registration and lookup support.", 
            "title": "Consul"
        }, 
        {
            "location": "/clustering/#worker", 
            "text": "Worker is a ViyaDB process that keeps a partition of data in RAM. Every such process is\nattached to it's own CPU core (using CPU affinity) for achieving better cache locality.  Every worker listens on its own HTTP port, where it provides a service for loading and\nquerying the data it keeps.", 
            "title": "Worker"
        }, 
        {
            "location": "/clustering/#controller", 
            "text": "Controller is the main process that launches ViyaDB workers on a single node, and takes\ncare of keeping them alive. In addition, this process is responsible for coordination\nbetween database nodes using Consul. One of cluster controllers becomes a leader.  Controller can act as query aggregator, which means that it can be queried directly, and\na query will be sent to workers containing required data partitions, then the result\nwill be combined and returned back to a user.  Communication with workers is performed using HTTP API.", 
            "title": "Controller"
        }, 
        {
            "location": "/clustering/#real-time-data-processing", 
            "text": "To see the complete real-time data processing architecture based on ViyaDB cluster, please\nvisit  next  section.", 
            "title": "Real-Time Data Processing"
        }, 
        {
            "location": "/realtime/", 
            "text": "Real-Time Architecture\n\n\nViyaDB implements shared nothing architecture, which means that workers hold partitions\nof data independently from each other. This allows for many possible implementations of\nreal-time analytics architecture depending on business requirements and/or restrictions\nyou might have.\n\n\nBelow is one of possible implementations of real-time ViyaDB environment.\n\n\nDiagram\n\n\n\n\nComponents\n\n\nLet's describe all the components of this particular data processing implementation, and how\nthey help gather insights off of hundreds of thousands of events per second in real-time.\n\n\nConsul\n\n\nConsul or technologies alike are irreplaceable in any distributed application, where different\ncomponents need agree on something. In our case, Consul will be used for two things:\n\n\n\n\nCentral configuration storage\n\n\nCoordination of ViyaDB cluster nodes\n\n\n\n\nThe quickest way to setup Consul is using Docker:\n\n\ndocker run --rm -ti --name viyadb-consul -p \n8500\n:8500 consul\n\n\n\n\n\nKafka\n\n\nKafka is a de facto standard messaging systeam in the industry for delivering events, thus it was\nchosen as the first supported data source. We leverage Kafka also for storing and exchanging various \nmetadata between different components.\n\n\nIf you don't have any Kafka, please start one using \nKafka Quickstart\n documentation.\n\n\nDeep Store\n\n\nDeep Store is a place on some kind of distributed file system or object store for keeping Indexer results.\nIn fact, the copy of all the data is always stored there, which facilitates data repair and data recovery.\n\n\n\n\nFile systems support\n\n\nFor now, only POSIX-compilant file systems and Amazon S3 are supported.\n\n\n\n\nIndexer\n\n\nThis component consists of two sub-components:\n\n\n\n\nReal-time processor\n\n\nBatch processor\n\n\n\n\nThe first component represents Spark Streaming based application whose task is to read events\nfrom Kafka, and pre-aggregate them into micro-batches for loading into ViyaDB.\nThe second part is invoked periodically, and it aims at compacting previous micro-batches\nand creating the best data partitioning plan.\n\n\nSpark cluster is required for running the Indexer at high scale. For testing purposes it's enough to just download\nand extract latest \nApache Spark\n package.\n\n\nTo read more about the Indexer, please refer to the \nREADME\n file\nof the project.\n\n\nViyaDB\n\n\nThis is the database cluster itself. Every physical node of the cluster consists of supervisor process,\nwhich spawns worker processes by the number of available CPU. Node controller running on every node\ndeals with loading data into worker processes. In addition, controller also redirects database queries to \na worker node containing required data partition, based on query filter (or queries needed workers,\nand aggregates results).\n\n\nConfiguration\n\n\nTable configuration\n\n\nThis configuration defines database table structure, what event fields represent dimensions,\nand which ones are metrics. See \nCreating Tables\n section for more\non table configuration.\n\n\n{\n\n  \nname\n:\n \nevents\n,\n\n  \ndimensions\n:\n \n[\n\n    \n{\nname\n:\n \napp_id\n},\n\n    \n{\nname\n:\n \nuser_id\n,\n \ntype\n:\n \nuint\n},\n\n    \n{\n\n      \nname\n:\n \nevent_time\n,\n \ntype\n:\n \ntime\n,\n\n      \nformat\n:\n \nmillis\n,\n \ngranularity\n:\n \nday\n\n    \n},\n\n    \n{\nname\n:\n \ncountry\n},\n\n    \n{\nname\n:\n \ncity\n},\n\n    \n{\nname\n:\n \ndevice_type\n},\n\n    \n{\nname\n:\n \ndevice_vendor\n},\n\n    \n{\nname\n:\n \nad_network\n},\n\n    \n{\nname\n:\n \ncampaign\n},\n\n    \n{\nname\n:\n \nsite_id\n},\n\n    \n{\nname\n:\n \nevent_type\n},\n\n    \n{\nname\n:\n \nevent_name\n},\n\n    \n{\nname\n:\n \norganic\n,\n \ncardinality\n:\n \n2\n},\n\n    \n{\nname\n:\n \ndays_from_install\n,\n \ntype\n:\n \nushort\n}\n\n  \n],\n\n  \nmetrics\n:\n \n[\n\n    \n{\nname\n:\n \nrevenue\n \n,\n \ntype\n:\n \ndouble_sum\n},\n\n    \n{\nname\n:\n \nusers\n,\n \ntype\n:\n \nbitset\n,\n \nfield\n:\n \nuser_id\n,\n \nmax\n:\n \n4294967295\n},\n\n    \n{\nname\n:\n \ncount\n \n,\n \ntype\n:\n \ncount\n}\n\n  \n]\n\n\n}\n\n\n\n\n\n\nTo load this configuration into Consul, store this JSON in file \ntable.json\n, and run:\n\n\ncurl --request PUT --data @table.json \n\\\n\n  http://\nconsul-host\n:8500/v1/kv/viyadb/tables/events/config\n\n\n\n\n\nIndexer configuration\n\n\nThe following section tells Indexer where to read data from, what's the data format\n(supported formats are: JSON, CSV), what are micro-batch and big batch window sizes, etc.\n\n\n{\n\n  \ntables\n:[\n\n    \nevents\n\n  \n],\n\n  \ndeepStorePath\n:\ns3://viyadb-deepstore\n,\n\n  \nrealTime\n:{\n\n    \nwindowDuration\n:\nPT15S\n,\n\n    \nkafkaSource\n:{\n\n      \ntopics\n:[\n\n        \nevents\n\n      \n],\n\n      \nbrokers\n:[\n\n        \nkafka-host:9092\n\n      \n]\n\n    \n},\n\n    \nparseSpec\n:{\n\n      \nformat\n:\njson\n,\n\n      \ntimeColumn\n:{\n\n        \nname\n:\nevent_time\n\n      \n}\n\n    \n},\n\n    \nnotifier\n:{\n\n      \ntype\n:\nkafka\n,\n\n      \nchannel\n:\nkafka-host:9092\n,\n\n      \nqueue\n:\nrt-notifications\n\n    \n}\n\n  \n},\n\n  \nbatch\n:{\n\n    \npartitioning\n:{\n\n      \ncolumns\n:[\n\n        \napp_id\n\n      \n],\n\n      \npartitions\n:\n16\n\n    \n},\n\n    \nnotifier\n:{\n\n      \ntype\n:\nkafka\n,\n\n      \nchannel\n:\nkafka-host:9092\n,\n\n      \nqueue\n:\nbatch-notifications\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nReplace \nkafka-host:9090\n with comma separated list of your actual Kafka brokers.\nTo load this configuration into Consul, store this JSON in file \nindexer.json\n, and run:\n\n\ncurl --request PUT --data @indexer.json \n\\\n\n  http://\nconsul-host\n:8500/v1/kv/viyadb/indexers/main/config\n\n\n\n\n\nCluster configuration\n\n\nThis configuration tells ViyaDB controller what tables to create, and which indexers to pull\nthe data from.\n\n\n{\n\n  \nreplication_factor\n:\n \n2\n,\n\n  \nworkers\n:\n \n32\n,\n\n  \ntables\n:\n \n[\nevents\n],\n\n  \nindexers\n:\n \n[\nmain\n]\n\n\n}\n\n\n\n\n\n\nCurrently, the number of expected workers must be set explicitly in cluster configuration.\nThis is the total number of nodes multiplied by the number of CPU cores on each machine.\nUsing this configuration, two copies of a single partition will be placed on two different workers.\n\n\nTo load this configuration into Consul, store this JSON in file \ncluster.json\n, and run:\n\n\ncurl --request PUT --data @cluster.json \n\\\n\n  http://\nconsul-host\n:8500/v1/kv/viyadb/clusters/cluster001/config\n\n\n\n\n\nStarting components\n\n\nMobile user activity simulator\n\n\nLet's run a simulator, which will inject some synthetic data representing mobile app\nuser activity into Kafka:\n\n\ndocker run --log-driver\n=\nnone --rm -ti viyadb/events-generator:latest \n|\n \n\\\n\n  kafka-console-producer.sh --broker-list \nkafka-host\n:9092 \n\\\n\n  --topic events \n/dev/null\n\n\n\n\n\nIndexer process\n\n\nOnce the configuration part is done, we can start the Indexer process. The following\ncommand is tuned for running Indexer in a limited Spark local mode environment,\nplease tune it according to your Spark cluster capacity and events rate.\n\n\nwget -q \n\\\n\n  https://github.com/viyadb/viyadb-spark/releases/download/v0.0.2/viyadb-spark_2.11-0.0.2-uberjar.jar\n\nspark-submit --executor-memory 2G \n\\\n\n  --conf spark.streaming.kafka.maxRatePerPartition\n=\n10000\n \n\\\n\n  --conf spark.sql.shuffle.partitions\n=\n1\n \n\\\n\n  --class com.github.viyadb.spark.streaming.Job \n\\\n\n  viyadb-spark_2.11-0.0.2-uberjar.jar \n\\\n\n  --indexer-id \nmain\n\n\n\n\n\n\nDatabase cluster\n\n\nNow it's the time to start ViyaDB cluster. Repeat the following procedure for every\nnode (we'll start 4 nodes):\n\n\nmkdir /tmp/viyadb\ncat \n/tmp/viyadb/conf.json \nEOF\n\n\n{\n\n\n  \nsupervise\n: true,\n\n\n  \nworkers\n: 8,\n\n\n  \ncluster_id\n: \ncluster001\n,\n\n\n  \nconsul_url\n: \nhttp://consul-host:8500\n,\n\n\n  \nstate_dir\n: \n/var/lib/viyadb\n\n\n}\n\n\nEOF\n\n\ndocker run --rm -ti \n\\\n\n  -p \n5000\n-5007:5000-5007 \n\\\n\n  -p \n5555\n:5555 \n\\\n\n  -v /tmp/viyadb:/var/lib/viyadb:rw \n\\\n\n  viyadb/viyadb:latest \n\\\n\n  /opt/viyadb/bin/viyad /var/lib/viyadb/conf.json\n\n\n\n\n\nQuerying\n\n\nOnce all components are started and running, we can start sending some queries.\n\n\nMultiple options to query ViyaDB exist, such as \nREST API\n\nor \nZeppelin\n interpreter,\nbut we'll just use SQL shell for this test.\n\n\nStart the shell, and provide it with a hostname of one of ViyaDB cluster nodes,\nand a controller port number:\n\n\ndocker run --rm -ti viyadb/viyadb:latest \n\\\n\n  /opt/viyadb/bin/vsql viyadb-host \n5555\n\n\n\n\n\n\nTop 10 cities by distinct users count\n\n\nViyaDB\n SELECT city, users FROM events WHERE \napp_id\n=\nkik.android\n and city \n \n ORDER BY users DESC LIMIT \n10\n;\n\n\ncity               users\n----               -----\nKowloon            \n76\n\nTsuen Wan          \n74\n\nYuen Long Kau Hui  \n70\n\nHong Kong          \n67\n\nAzur               \n9\n\nQiryat Ono         \n9\n\nTayibe             \n9\n\nGanne Tiqwa        \n8\n\nSederot            \n7\n\nAbu Ghosh          \n7\n\n\nTime taken: \n0\n.012320 secs\n\n\n\n\n\nUser retention query\n\n\nViyaDB\n SELECT ad_network, days_from_install, users FROM events WHERE \napp_id\n=\nkik.android\n AND event_time BETWEEN \n2015-01-01\n AND \n2015-01-31\n AND \nevent_type\n=\nsession\n AND ad_network IN\n(\nFacebook\n, \nGoogle\n, \nTwitter\n)\n ORDER BY days_from_install\n;\n\n\nad_network  days_from_install  users\n----------  -----------------  -----\nTwitter     \n0\n                  \n573\n\nGoogle      \n0\n                  \n623\n\nFacebook    \n0\n                  \n595\n\nGoogle      \n1\n                  \n427\n\nFacebook    \n1\n                  \n410\n\nTwitter     \n1\n                  \n398\n\nTwitter     \n2\n                  \n263\n\nGoogle      \n2\n                  \n250\n\nFacebook    \n2\n                  \n253\n\nTwitter     \n3\n                  \n112\n\nFacebook    \n3\n                  \n105\n\nGoogle      \n3\n                  \n106\n\nTwitter     \n4\n                  \n9\n\nFacebook    \n4\n                  \n8\n\nGoogle      \n4\n                  \n12\n\n\nTime taken: \n0\n.057661 secs\n\n\n\n\n\nWork in progress\n\n\nThis is just the beginning, and most functionality like cluster management and monitoring is still missing,\nbut we're working on it.", 
            "title": "Realtime"
        }, 
        {
            "location": "/realtime/#real-time-architecture", 
            "text": "ViyaDB implements shared nothing architecture, which means that workers hold partitions\nof data independently from each other. This allows for many possible implementations of\nreal-time analytics architecture depending on business requirements and/or restrictions\nyou might have.  Below is one of possible implementations of real-time ViyaDB environment.", 
            "title": "Real-Time Architecture"
        }, 
        {
            "location": "/realtime/#diagram", 
            "text": "", 
            "title": "Diagram"
        }, 
        {
            "location": "/realtime/#components", 
            "text": "Let's describe all the components of this particular data processing implementation, and how\nthey help gather insights off of hundreds of thousands of events per second in real-time.", 
            "title": "Components"
        }, 
        {
            "location": "/realtime/#consul", 
            "text": "Consul or technologies alike are irreplaceable in any distributed application, where different\ncomponents need agree on something. In our case, Consul will be used for two things:   Central configuration storage  Coordination of ViyaDB cluster nodes   The quickest way to setup Consul is using Docker:  docker run --rm -ti --name viyadb-consul -p  8500 :8500 consul", 
            "title": "Consul"
        }, 
        {
            "location": "/realtime/#kafka", 
            "text": "Kafka is a de facto standard messaging systeam in the industry for delivering events, thus it was\nchosen as the first supported data source. We leverage Kafka also for storing and exchanging various \nmetadata between different components.  If you don't have any Kafka, please start one using  Kafka Quickstart  documentation.", 
            "title": "Kafka"
        }, 
        {
            "location": "/realtime/#deep-store", 
            "text": "Deep Store is a place on some kind of distributed file system or object store for keeping Indexer results.\nIn fact, the copy of all the data is always stored there, which facilitates data repair and data recovery.   File systems support  For now, only POSIX-compilant file systems and Amazon S3 are supported.", 
            "title": "Deep Store"
        }, 
        {
            "location": "/realtime/#indexer", 
            "text": "This component consists of two sub-components:   Real-time processor  Batch processor   The first component represents Spark Streaming based application whose task is to read events\nfrom Kafka, and pre-aggregate them into micro-batches for loading into ViyaDB.\nThe second part is invoked periodically, and it aims at compacting previous micro-batches\nand creating the best data partitioning plan.  Spark cluster is required for running the Indexer at high scale. For testing purposes it's enough to just download\nand extract latest  Apache Spark  package.  To read more about the Indexer, please refer to the  README  file\nof the project.", 
            "title": "Indexer"
        }, 
        {
            "location": "/realtime/#viyadb", 
            "text": "This is the database cluster itself. Every physical node of the cluster consists of supervisor process,\nwhich spawns worker processes by the number of available CPU. Node controller running on every node\ndeals with loading data into worker processes. In addition, controller also redirects database queries to \na worker node containing required data partition, based on query filter (or queries needed workers,\nand aggregates results).", 
            "title": "ViyaDB"
        }, 
        {
            "location": "/realtime/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/realtime/#table-configuration", 
            "text": "This configuration defines database table structure, what event fields represent dimensions,\nand which ones are metrics. See  Creating Tables  section for more\non table configuration.  { \n   name :   events , \n   dimensions :   [ \n     { name :   app_id }, \n     { name :   user_id ,   type :   uint }, \n     { \n       name :   event_time ,   type :   time , \n       format :   millis ,   granularity :   day \n     }, \n     { name :   country }, \n     { name :   city }, \n     { name :   device_type }, \n     { name :   device_vendor }, \n     { name :   ad_network }, \n     { name :   campaign }, \n     { name :   site_id }, \n     { name :   event_type }, \n     { name :   event_name }, \n     { name :   organic ,   cardinality :   2 }, \n     { name :   days_from_install ,   type :   ushort } \n   ], \n   metrics :   [ \n     { name :   revenue   ,   type :   double_sum }, \n     { name :   users ,   type :   bitset ,   field :   user_id ,   max :   4294967295 }, \n     { name :   count   ,   type :   count } \n   ]  }   To load this configuration into Consul, store this JSON in file  table.json , and run:  curl --request PUT --data @table.json  \\ \n  http:// consul-host :8500/v1/kv/viyadb/tables/events/config", 
            "title": "Table configuration"
        }, 
        {
            "location": "/realtime/#indexer-configuration", 
            "text": "The following section tells Indexer where to read data from, what's the data format\n(supported formats are: JSON, CSV), what are micro-batch and big batch window sizes, etc.  { \n   tables :[ \n     events \n   ], \n   deepStorePath : s3://viyadb-deepstore , \n   realTime :{ \n     windowDuration : PT15S , \n     kafkaSource :{ \n       topics :[ \n         events \n       ], \n       brokers :[ \n         kafka-host:9092 \n       ] \n     }, \n     parseSpec :{ \n       format : json , \n       timeColumn :{ \n         name : event_time \n       } \n     }, \n     notifier :{ \n       type : kafka , \n       channel : kafka-host:9092 , \n       queue : rt-notifications \n     } \n   }, \n   batch :{ \n     partitioning :{ \n       columns :[ \n         app_id \n       ], \n       partitions : 16 \n     }, \n     notifier :{ \n       type : kafka , \n       channel : kafka-host:9092 , \n       queue : batch-notifications \n     } \n   }  }   Replace  kafka-host:9090  with comma separated list of your actual Kafka brokers.\nTo load this configuration into Consul, store this JSON in file  indexer.json , and run:  curl --request PUT --data @indexer.json  \\ \n  http:// consul-host :8500/v1/kv/viyadb/indexers/main/config", 
            "title": "Indexer configuration"
        }, 
        {
            "location": "/realtime/#cluster-configuration", 
            "text": "This configuration tells ViyaDB controller what tables to create, and which indexers to pull\nthe data from.  { \n   replication_factor :   2 , \n   workers :   32 , \n   tables :   [ events ], \n   indexers :   [ main ]  }   Currently, the number of expected workers must be set explicitly in cluster configuration.\nThis is the total number of nodes multiplied by the number of CPU cores on each machine.\nUsing this configuration, two copies of a single partition will be placed on two different workers.  To load this configuration into Consul, store this JSON in file  cluster.json , and run:  curl --request PUT --data @cluster.json  \\ \n  http:// consul-host :8500/v1/kv/viyadb/clusters/cluster001/config", 
            "title": "Cluster configuration"
        }, 
        {
            "location": "/realtime/#starting-components", 
            "text": "", 
            "title": "Starting components"
        }, 
        {
            "location": "/realtime/#mobile-user-activity-simulator", 
            "text": "Let's run a simulator, which will inject some synthetic data representing mobile app\nuser activity into Kafka:  docker run --log-driver = none --rm -ti viyadb/events-generator:latest  |   \\ \n  kafka-console-producer.sh --broker-list  kafka-host :9092  \\ \n  --topic events  /dev/null", 
            "title": "Mobile user activity simulator"
        }, 
        {
            "location": "/realtime/#indexer-process", 
            "text": "Once the configuration part is done, we can start the Indexer process. The following\ncommand is tuned for running Indexer in a limited Spark local mode environment,\nplease tune it according to your Spark cluster capacity and events rate.  wget -q  \\ \n  https://github.com/viyadb/viyadb-spark/releases/download/v0.0.2/viyadb-spark_2.11-0.0.2-uberjar.jar\n\nspark-submit --executor-memory 2G  \\ \n  --conf spark.streaming.kafka.maxRatePerPartition = 10000   \\ \n  --conf spark.sql.shuffle.partitions = 1   \\ \n  --class com.github.viyadb.spark.streaming.Job  \\ \n  viyadb-spark_2.11-0.0.2-uberjar.jar  \\ \n  --indexer-id  main", 
            "title": "Indexer process"
        }, 
        {
            "location": "/realtime/#database-cluster", 
            "text": "Now it's the time to start ViyaDB cluster. Repeat the following procedure for every\nnode (we'll start 4 nodes):  mkdir /tmp/viyadb\ncat  /tmp/viyadb/conf.json  EOF  {     supervise : true,     workers : 8,     cluster_id :  cluster001 ,     consul_url :  http://consul-host:8500 ,     state_dir :  /var/lib/viyadb  }  EOF \n\ndocker run --rm -ti  \\ \n  -p  5000 -5007:5000-5007  \\ \n  -p  5555 :5555  \\ \n  -v /tmp/viyadb:/var/lib/viyadb:rw  \\ \n  viyadb/viyadb:latest  \\ \n  /opt/viyadb/bin/viyad /var/lib/viyadb/conf.json", 
            "title": "Database cluster"
        }, 
        {
            "location": "/realtime/#querying", 
            "text": "Once all components are started and running, we can start sending some queries.  Multiple options to query ViyaDB exist, such as  REST API \nor  Zeppelin  interpreter,\nbut we'll just use SQL shell for this test.  Start the shell, and provide it with a hostname of one of ViyaDB cluster nodes,\nand a controller port number:  docker run --rm -ti viyadb/viyadb:latest  \\ \n  /opt/viyadb/bin/vsql viyadb-host  5555", 
            "title": "Querying"
        }, 
        {
            "location": "/realtime/#top-10-cities-by-distinct-users-count", 
            "text": "ViyaDB  SELECT city, users FROM events WHERE  app_id = kik.android  and city     ORDER BY users DESC LIMIT  10 ; \n\ncity               users\n----               -----\nKowloon             76 \nTsuen Wan           74 \nYuen Long Kau Hui   70 \nHong Kong           67 \nAzur                9 \nQiryat Ono          9 \nTayibe              9 \nGanne Tiqwa         8 \nSederot             7 \nAbu Ghosh           7 \n\nTime taken:  0 .012320 secs", 
            "title": "Top 10 cities by distinct users count"
        }, 
        {
            "location": "/realtime/#user-retention-query", 
            "text": "ViyaDB  SELECT ad_network, days_from_install, users FROM events WHERE  app_id = kik.android  AND event_time BETWEEN  2015-01-01  AND  2015-01-31  AND  event_type = session  AND ad_network IN ( Facebook ,  Google ,  Twitter )  ORDER BY days_from_install ; \n\nad_network  days_from_install  users\n----------  -----------------  -----\nTwitter      0                    573 \nGoogle       0                    623 \nFacebook     0                    595 \nGoogle       1                    427 \nFacebook     1                    410 \nTwitter      1                    398 \nTwitter      2                    263 \nGoogle       2                    250 \nFacebook     2                    253 \nTwitter      3                    112 \nFacebook     3                    105 \nGoogle       3                    106 \nTwitter      4                    9 \nFacebook     4                    8 \nGoogle       4                    12 \n\nTime taken:  0 .057661 secs", 
            "title": "User retention query"
        }, 
        {
            "location": "/realtime/#work-in-progress", 
            "text": "This is just the beginning, and most functionality like cluster management and monitoring is still missing,\nbut we're working on it.", 
            "title": "Work in progress"
        }
    ]
}